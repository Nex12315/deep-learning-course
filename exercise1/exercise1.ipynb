{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "910b5f33",
   "metadata": {},
   "source": [
    "#### Imports and initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74aa7f7f-af79-453d-8217-265865bd529d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn import metrics\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c33335e5-2bd3-409d-bf38-06fb4cfd8c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed for reproducibility\n",
    "np.random.seed(228)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a83ec82",
   "metadata": {},
   "source": [
    "#### Step 1: Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "911d75d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the file\n",
    "df = pd.read_csv('coin_Bitcoin.csv')\n",
    "\n",
    "# Display basic information\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(\"\\nFirst 5 rows:\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c1f7a0e",
   "metadata": {},
   "source": [
    "#### Step 2: Clean up the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a22ed15e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print(\"\\nMissing values:\")\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5cfc30d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for zero values in Volume (potentially problematic)\n",
    "zero_volume_count = (df['Volume'] == 0).sum()\n",
    "print(f\"\\nRows with zero Volume: {zero_volume_count} ({zero_volume_count/len(df)*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f59f69f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace zero volumes with the mean of non-zero volumes\n",
    "non_zero_volumes = df[df['Volume'] > 0]['Volume']\n",
    "mean_volume = non_zero_volumes.mean()\n",
    "df.loc[df['Volume'] == 0, 'Volume'] = mean_volume\n",
    "print(f\"Replaced zero volumes with mean value: {mean_volume:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "025ad4e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Date to datetime and extract useful features\n",
    "df['Date'] = pd.to_datetime(df['Date'])\n",
    "df['Year'] = df['Date'].dt.year\n",
    "df['Month'] = df['Date'].dt.month\n",
    "df['Day'] = df['Date'].dt.day\n",
    "df['DayOfWeek'] = df['Date'].dt.dayofweek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db342bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate days since the start\n",
    "df['DaysSinceStart'] = (df['Date'] - df['Date'].min()).dt.days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af1c34c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "While not explicitly required by Step 2, I added several engineered features that are helpful for financial price prediction:\n",
    "\n",
    "Price changes (absolute and percentage)\n",
    "Daily price range (high minus low)\n",
    "Moving averages (7-day and 30-day)\n",
    "Distance from moving averages\n",
    "\n",
    "These features incorporate domain knowledge from financial technical analysis and help the model understand price trends and volatility.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89825c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate price changes and other features\n",
    "df['PriceChange'] = df['Close'].diff()\n",
    "df['PriceChangePercent'] = df['Close'].pct_change() * 100\n",
    "df['DailyRange'] = df['High'] - df['Low']\n",
    "df['DailyRangePercent'] = df['DailyRange'] / df['Open'] * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c28098",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add moving averages\n",
    "df['MA7'] = df['Close'].rolling(window=7).mean()\n",
    "df['MA30'] = df['Close'].rolling(window=30).mean()\n",
    "df['DistFromMA7'] = df['Close'] - df['MA7']\n",
    "df['DistFromMA7Percent'] = df['DistFromMA7'] / df['MA7'] * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d56180c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows with NaN values (from rolling calculations)\n",
    "df_clean = df.dropna()\n",
    "print(f\"\\nDataset size after cleaning: {df_clean.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2cd4024",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploring data and visualizing important patterns\n",
    "\n",
    "# Plot Bitcoin price over time\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(df_clean['Date'], df_clean['Close'])\n",
    "plt.title('Bitcoin Price History')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Price (USD)')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a9d357",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate correlation matrix\n",
    "correlation_matrix = df_clean[['Open', 'High', 'Low', 'Close', 'Volume', 'Marketcap', \n",
    "                         'MA7', 'MA30', 'DailyRange', 'PriceChange']].corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab98973",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot correlation heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f')\n",
    "plt.title('Correlation Matrix')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "629579a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nCorrelation with Close price:\")\n",
    "print(correlation_matrix['Close'].sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61162aca",
   "metadata": {},
   "source": [
    "#### Step 3: Select features and prepare for modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c7ddfdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the features to use based on correlation and domain knowledge\n",
    "features = ['Open', 'High', 'Low', 'Volume', 'MA7', 'DailyRange', \n",
    "            'Year', 'Month', 'DayOfWeek', 'DaysSinceStart']\n",
    "\n",
    "# Define X (features) and y (target)\n",
    "X = df_clean[features]\n",
    "y = df_clean['Close']\n",
    "\n",
    "# Scale the features to improve training\n",
    "scaler_X = MinMaxScaler()\n",
    "scaler_y = MinMaxScaler()\n",
    "\n",
    "X_scaled = scaler_X.fit_transform(X)\n",
    "y_scaled = scaler_y.fit_transform(y.values.reshape(-1, 1)).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b865fa2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into training and testing sets\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_scaled, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Training set: {X_train.shape[0]} samples\")\n",
    "print(f\"Test set: {X_test.shape[0]} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bba072c",
   "metadata": {},
   "source": [
    "#### Step 4: Create the neural network model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c197b795",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Create the neural network model\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(64, activation='relu', input_shape=(X_train.shape[1],)))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(16, activation='relu'))\n",
    "model.add(Dense(1))\n",
    "\n",
    "# Compile the model\n",
    "optimizer = Adam(learning_rate=0.001)\n",
    "model.compile(optimizer=optimizer, loss='mse', metrics=['mae'])\n",
    "\n",
    "# Print model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12925da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "\n",
    "# Early stopping to prevent overfitting\n",
    "early_stop = EarlyStopping(\n",
    "    monitor='val_loss', \n",
    "    patience=20, \n",
    "    restore_best_weights=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6667e5a",
   "metadata": {},
   "source": [
    "#### Step 5: Fit your data to the neural network model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d0dd7b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_split=0.2, \n",
    "    epochs=100, \n",
    "    batch_size=32, \n",
    "    callbacks=[checkpoint_callback, early_stop],\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03d3a465",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model Loss')\n",
    "plt.ylabel('Loss (MSE)')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper right')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abfaac23",
   "metadata": {},
   "source": [
    "#### Step 6: Inspect the model with error metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9bdb0fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot mean absolute error\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['mae'])\n",
    "plt.plot(history.history['val_mae'])\n",
    "plt.title('Model MAE')\n",
    "plt.ylabel('MAE')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper right')\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9124c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test data\n",
    "test_loss, test_mae = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"Test loss (MSE): {test_loss:.6f}\")\n",
    "print(f\"Test MAE: {test_mae:.6f}\")\n",
    "\n",
    "# Make predictions\n",
    "y_pred_scaled = model.predict(X_test)\n",
    "y_pred = scaler_y.inverse_transform(y_pred_scaled)\n",
    "y_test_original = scaler_y.inverse_transform(y_test.reshape(-1, 1))\n",
    "\n",
    "# Calculate regression metrics on the original scale\n",
    "mse = metrics.mean_squared_error(y_test_original, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "mae = metrics.mean_absolute_error(y_test_original, y_pred)\n",
    "r2 = metrics.r2_score(y_test_original, y_pred)\n",
    "\n",
    "print(\"\\nRegression Metrics (on original scale):\")\n",
    "print(f\"Mean Squared Error (MSE): {mse:.2f}\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse:.2f}\")\n",
    "print(f\"Mean Absolute Error (MAE): {mae:.2f}\")\n",
    "print(f\"R-squared (R²): {r2:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45e6da0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize predictions vs actual\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Create indices for test samples\n",
    "indices = np.arange(len(y_test_original))\n",
    "\n",
    "# Select a subset if too many points\n",
    "plot_indices = indices\n",
    "if len(indices) > 100:\n",
    "    plot_indices = np.random.choice(indices, 100, replace=False)\n",
    "    plot_indices.sort()\n",
    "\n",
    "plt.scatter(indices[plot_indices], y_test_original[plot_indices], color='blue', label='Actual')\n",
    "plt.scatter(indices[plot_indices], y_pred[plot_indices], color='red', label='Predicted')\n",
    "plt.title('Bitcoin Price: Predicted vs Actual')\n",
    "plt.xlabel('Test Sample Index')\n",
    "plt.ylabel('Price (USD)')\n",
    "plt.legend()\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e46eacea",
   "metadata": {},
   "source": [
    "#### Step 7: Try the model with some new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d86eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get one of the last samples as a test case\n",
    "sample_data = X[-1:].copy()  # Last row of the original dataset\n",
    "sample_data_scaled = scaler_X.transform(sample_data)\n",
    "sample_prediction_scaled = model.predict(sample_data_scaled)\n",
    "sample_prediction = scaler_y.inverse_transform(sample_prediction_scaled)[0][0]\n",
    "\n",
    "# Get the actual value\n",
    "actual_price = df_clean['Close'].iloc[-1]\n",
    "\n",
    "print(f\"Sample input features: {df_clean[features].iloc[-1].to_dict()}\")\n",
    "print(f\"Predicted price: ${sample_prediction:.2f}\")\n",
    "print(f\"Actual price: ${actual_price:.2f}\")\n",
    "print(f\"Difference: ${abs(sample_prediction - actual_price):.2f}\")\n",
    "print(f\"Percentage error: {abs(sample_prediction - actual_price) / actual_price * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1929ae92",
   "metadata": {},
   "source": [
    "#### Step 8: Final thoughts and conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "28242e02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nIn working life, regression models like this have numerous practical applications:\\n\\n- Financial forecasting for investment decisions\\n- Demand prediction for inventory management\\n- Resource allocation in project planning\\n- Predictive maintenance in manufacturing\\n- Sales forecasting for business planning\\n\\nKeras made implementation relatively straightforward compared to building models from scratch. \\nThe high-level API handles the complex mathematical operations behind the scenes, allowing us to focus on model architecture and feature engineering.\\n\\nThe model achieved impressive performance with an R² value close to 1, but there's always room for improvement:\\n\\n- Feature engineering: We could incorporate external factors like market sentiment, Google Trends data, or macroeconomic indicators.\\n- Advanced architectures: LSTM or GRU networks might better capture temporal dependencies in price data.\\n- Hyperparameter tuning: Grid search or Bayesian optimization could find optimal learning rates, layer sizes, and activation functions.\\n- Ensemble methods: Combining predictions from multiple models often yields better results than any single model.\\n\""
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "In working life, regression models like this have numerous practical applications:\n",
    "\n",
    "- Financial forecasting for investment decisions\n",
    "- Demand prediction for inventory management\n",
    "- Resource allocation in project planning\n",
    "- Predictive maintenance in manufacturing\n",
    "- Sales forecasting for business planning\n",
    "\n",
    "Keras made implementation relatively straightforward compared to building models from scratch. \n",
    "The high-level API handles the complex mathematical operations behind the scenes, allowing us to focus on model architecture and feature engineering.\n",
    "\n",
    "The model achieved impressive performance with an R² value close to 1, but there's always room for improvement:\n",
    "\n",
    "- Feature engineering: We could incorporate external factors like market sentiment, Google Trends data, or macroeconomic indicators.\n",
    "- Advanced architectures: LSTM or GRU networks might better capture temporal dependencies in price data.\n",
    "- Hyperparameter tuning: Grid search or Bayesian optimization could find optimal learning rates, layer sizes, and activation functions.\n",
    "- Ensemble methods: Combining predictions from multiple models often yields better results than any single model.\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
